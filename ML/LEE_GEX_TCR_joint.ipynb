{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCR-GEX Joint Analysis\n",
        "\n",
        "This notebook performs joint analysis of T-cell receptor (TCR) and gene expression (GEX) data to predict tissue localization.\n",
        "\n",
        "**Created:** Fri Aug 8 14:24:07 2025  \n",
        "**Author:** a4945\n",
        "\n",
        "## Overview\n",
        "This script predicts tissue localization using:\n",
        "- DeepTCR embeddings\n",
        "- Gene expression features  \n",
        "- Chemokine profiles\n",
        "- T-cell state information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Aug  8 14:24:07 2025\n",
        "\n",
        "@author: a4945\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Feb 27 14:03:00 2025\n",
        "\n",
        "# this script predict the \n",
        "\n",
        "@author: a4945\n",
        "\"\"\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold\n",
        "torch.manual_seed(455)\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(455)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "\n",
        "Load TCR representations, T-cell states, and DeepTCR embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% load deepTCR embeding\n",
        "tcr_rep = pd.read_csv(\"../TCR/tcr_features.csv\", delimiter=\",\")\n",
        "# tcr_rep = tcr_rep.dropna()\n",
        "\n",
        "TCRdist_MOG = pd.read_csv(\"../TCR/TCRdist_MOG.csv\", delimiter=\",\")\n",
        "TCRdist_MOG['cdr3_b_aa'] = TCRdist_MOG['cdr3_b_aa'].str[1:-1]\n",
        "TCRdist_MOG = TCRdist_MOG[['cdr3_b_aa', 'TCRdist_MOG']]\n",
        "\n",
        "#%%\n",
        "T_states = pd.read_csv(\"../TCR/T_states.csv\", delimiter=\",\")\n",
        "#### error in data labeling   ####\n",
        "T_states['clone_id_size'] = T_states['clone_id_size'].astype('int')\n",
        "T_states['state'].replace({\"Effector\": 'Naive/Mem', \"Mem\": 'Naive/Mem'}, inplace = True) \n",
        "\n",
        "tcr_rep = tcr_rep.merge(T_states, left_on='cell_id', right_on = 'Unnamed: 0', how ='inner')\n",
        "tcr_rep = tcr_rep.merge(TCRdist_MOG, left_on='VDJ_1_cdr3_aa', right_on = 'cdr3_b_aa', how ='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%  load DeepTCR embedings\n",
        "matrix = pd.read_csv('../TCR/DTCRU_extracted_features_96.csv', sep = ',')\n",
        "matrix.drop(columns={'Label'}, inplace=True)   # un useful col\n",
        "matrix['CDR3_Beta'] = matrix['CDR3_Beta'].str[1:-1]    # remove first C and last F in AA\n",
        "\n",
        "merged = pd.merge(tcr_rep, matrix, how='inner', left_on='VDJ_1_cdr3_aa', right_on='CDR3_Beta')\n",
        "merged = merged[(merged['VDJ_1_v_call'] == merged['V_Beta']) & \n",
        "                (merged['VDJ_1_j_call'] == merged['J_Beta'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gene Expression Data Processing\n",
        "\n",
        "Load and process gene expression features and chemokine profiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% process the gex features\n",
        "cat_gex = pd.read_csv(\"../TCR/gex_obs_classes.csv\", delimiter=\",\")   # all 0605 data\n",
        "cat_gex = cat_gex.iloc[:,0:6]\n",
        "cat_gex = cat_gex.dropna()\n",
        "cat_gex.rename(columns={'Unnamed: 0': 'cell_id'}, inplace=True)\n",
        "\n",
        "cat_gex['date'] = cat_gex['cell_id'].str.split('_').str[1]\n",
        "\n",
        "ID_0516 = {'CMO301': '5_3', 'CMO302': '5_4', 'CMO303':'5_5', \n",
        "               'CMO304':'5_6', 'CMO305': '5_7', 'CMO317': '5_8',\n",
        "               'CMO318': '5_3', 'CMO325':'5_4', 'CMO326':'5_5', \n",
        "               'CMO321':'5_6', 'CMO322': '5_7', 'CMO323': '5_8'}\n",
        "\n",
        "ID_0605 = {'CMO301': '6_1', 'CMO302': '6_2', 'CMO303':'6_3', 'CMO304':'6_4',\n",
        "           'CMO317': '6_1', 'CMO318':'6_2', 'CMO325':'6_3', 'CMO326':'6_4'}\n",
        "\n",
        "cat_gex['mouse_id'] = np.where(\n",
        "    cat_gex['date'] == '0605',\n",
        "    cat_gex['mouse_id'].map(ID_0605),\n",
        "    cat_gex['mouse_id'].map(ID_0516)\n",
        ")\n",
        "\n",
        "dup_cols = merged.columns.intersection(cat_gex.columns)\n",
        "cat_gex_clean = cat_gex.drop(columns=dup_cols)    \n",
        "df_all_features = pd.merge(cat_gex, merged, how='inner', on='cell_id')\n",
        "df_all_features.drop(columns='tissue_y', inplace=True)\n",
        "df_all_features.rename(columns={'tissue_x':'tissue'}, inplace=True)\n",
        "\n",
        "df_all_features['mouse_id'] = df_all_features['mouse_id'].astype('category')\n",
        "mouse_id_cats = df_all_features['mouse_id'].astype('category').cat.categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% load top 10 chemokine genes\n",
        "chemo_profile = pd.read_csv('../TCR/top10_chemok.csv')\n",
        "df_all_features = df_all_features.merge(chemo_profile, left_on='cell_id', right_on='Unnamed: 0')\n",
        "\n",
        "#%%  subsets\n",
        "df_all_features['is_cloned'] = df_all_features.duplicated('CDR3_Beta', keep=False)\n",
        "df_all_features['two_sites'] = df_all_features.groupby('CDR3_Beta')['tissue'].transform(lambda x: x.nunique() > 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Define preprocessing functions for feature engineering and data preparation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% data pre-processing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocessing(df_in, target):\n",
        "    str_cols = df_in.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns\n",
        "\n",
        "    # One-hot encode those, keep numeric columns as-is\n",
        "    df = pd.get_dummies(df_in, columns = str_cols, dtype=\"uint8\", dummy_na=True)   \n",
        "    df.columns = df.columns.astype(str)\n",
        "    \n",
        "    feature_names = df.columns\n",
        "    \n",
        "    # resampling\n",
        "    ros = RandomOverSampler(random_state=0)\n",
        "    X_resampled, Y_resampled = ros.fit_resample(df, target)\n",
        "\n",
        "    return X_resampled, Y_resampled, feature_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "Define the TCR classifier neural network model with regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% build model\n",
        "class TCRClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(TCRClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 64)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.output = nn.Linear(16, num_classes)\n",
        "        \n",
        "        # L1 and L2 regularization equivalent\n",
        "        self.l1_l2_reg = 0.01\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.layer3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.softmax(self.output(x), dim=1)\n",
        "        return x\n",
        "    \n",
        "    def l1_l2_loss(self):\n",
        "        l1_loss = sum(torch.norm(p, 1) for p in self.parameters())\n",
        "        l2_loss = sum(torch.norm(p, 2) for p in self.parameters())\n",
        "        return self.l1_l2_reg * (l1_loss + l2_loss)\n",
        "\n",
        "def build_model(input_size, num_classes):\n",
        "    model = TCRClassifier(input_size, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Functions\n",
        "\n",
        "Define functions for plotting confusion matrix and ROC curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%      plot confusion matrix  \n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "def plot_confusion(Y_test, class_pred, s, title):\n",
        "    cm = confusion_matrix(Y_test, class_pred)\n",
        "    s = merged[target_class].astype('category')\n",
        "    class_labels = s.cat.categories\n",
        "    pred_accuracy = (cm[0,0] + cm[1,1]) / np.sum(cm)\n",
        "    \n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = class_labels)\n",
        "    disp.plot(cmap='Blues')\n",
        "    plt.title(str(title) + \"  acc:\" + str(round(pred_accuracy, 3)))\n",
        "    plt.savefig(str(title) + \"_confusion.png\")\n",
        "    plt.show()\n",
        "\n",
        "## plot AUC \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ROC(Y_test, test_pred, title):\n",
        "    Y_test.iloc[-1] = 0\n",
        "    # y_true: true binary labels (0 or 1)\n",
        "    # y_scores: predicted probabilities for class 1 (NOT class labels)\n",
        "    # e.g. from model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, test_pred[:,1])\n",
        "    auc = roc_auc_score(Y_test, test_pred[:,1])\n",
        "    \n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(str(title))\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(str(title) + \"_ROC.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "Train the TCR classifier model and evaluate performance across different cell types and clonality conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%  train\n",
        "# select features\n",
        "input_cat_features = ['manual_cell_type', 'state', 'clone_id_size', 'TCRdist_MOG' ]   #    \n",
        "\n",
        "input_embs = [str(s) for s in range(94)]\n",
        "chemo_keys = chemo_profile.columns.values[1:].tolist()\n",
        "epoch_num = 100\n",
        "\n",
        "# check cols\n",
        "# df_all_features.columns\n",
        "# subset\n",
        "df_all_features = df_all_features[df_all_features['TCRdist_MOG'] < 100]\n",
        "\n",
        "Bool_list = [True]\n",
        "Cell_types = ['CD4+ T', 'CD8+ T', 'Treg']\n",
        "#     , 'Treg'\n",
        "\n",
        "target_class = 'tissue'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ind1 in Bool_list:\n",
        "    M_sub1 = df_all_features[df_all_features['is_cloned'] == ind1]\n",
        "    # M_sub1 = df_all_features                     # Not subsetting\n",
        "    \n",
        "    for ind2 in Cell_types:\n",
        "        M_sub = M_sub1[M_sub1['manual_cell_type'] == ind2]\n",
        "        # M_sub = M_sub1                          # Not subsetting\n",
        "        \n",
        "        mouse_id = str(ind1) + ind2        \n",
        "        features = M_sub[input_cat_features + input_embs + chemo_keys]      # \n",
        "        \n",
        "        num_classes = merged[target_class].astype('category').value_counts().shape[0]\n",
        "        target = M_sub[target_class].astype('category').cat.codes\n",
        "        s = M_sub[target_class]\n",
        "        \n",
        "        test_id = ['6_4', '5_5']\n",
        "        test_idx = M_sub['mouse_id'].isin(test_id)\n",
        "        \n",
        "        features_train = features[~test_idx]\n",
        "        target_train = target[~test_idx]\n",
        "        X_train, Y_train, _ = preprocessing(features_train, target_train)\n",
        "        \n",
        "        features_test = features[test_idx]\n",
        "        target_test = target[test_idx]\n",
        "        X_test, Y_test, feature_names = preprocessing(features_test, target_test)\n",
        "        \n",
        "        num_features = X_train.shape[1]\n",
        "        \n",
        "        # Convert to PyTorch tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train.values)\n",
        "        Y_train_tensor = torch.LongTensor(Y_train.values)\n",
        "        X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "        Y_test_tensor = torch.LongTensor(Y_test.values)\n",
        "        \n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        \n",
        "        # Build and train model\n",
        "        model = build_model(num_features, num_classes)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "        \n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for epoch in range(epoch_num):\n",
        "            for batch_X, batch_Y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_Y) + model.l1_l2_loss()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "        \n",
        "        #% test\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_pred = model(X_test_tensor).numpy()\n",
        "        class_pred = np.argmax(test_pred, axis=1)\n",
        "        \n",
        "        plot_confusion(Y_test, class_pred, s, mouse_id)\n",
        "        plot_ROC(Y_test, test_pred, mouse_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving and Loading\n",
        "\n",
        "Optional code for saving and loading trained models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%  save model\n",
        "# torch.save(model.state_dict(), 'TCR.pth')\n",
        "           \n",
        "# model = build_model(num_features, num_classes)\n",
        "# model.load_state_dict(torch.load('TCR.pth'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP Analysis\n",
        "\n",
        "Model interpretability analysis using SHAP to understand feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% shap explain\n",
        "# explain all the predictions in the test set\n",
        "\n",
        "# import shap\n",
        "# def shap_eavl(X_train, X_test, features):\n",
        "    \n",
        "    # Background (masker) — sample to keep things fast and stable\n",
        "rng = np.random.default_rng(0)\n",
        "bg_idx = rng.choice(X_train.shape[0], size=min(100, X_train.shape[0]), replace=False)\n",
        "background = X_train.iloc[bg_idx]\n",
        "\n",
        "# Prediction function that includes preprocessing if you want to explain raw X\n",
        "# Here we already precomputed X_train_s/X_test_s; if you'd rather pass raw X to SHAP,\n",
        "# define: f = lambda data: model.predict(scaler.transform(data), verbose=0)\n",
        "def predict_function(data):\n",
        "    data_tensor = torch.FloatTensor(data.values)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        return model(data_tensor).numpy()\n",
        "\n",
        "f = predict_function\n",
        "\n",
        "# Create the explainer (auto picks a fast, gradient-based method for TF/Keras when possible)\n",
        "explainer = shap.Explainer(f, shap.maskers.Independent(background))\n",
        "\n",
        "# Use a manageable slice for speed (e.g., 500 samples)\n",
        "sample_idx = rng.choice(X_test.shape[0], size=min(30, X_test.shape[0]), replace=False)\n",
        "X_eval = X_test.iloc[sample_idx]\n",
        "\n",
        "# Compute explanations\n",
        "shap_values = explainer(X_eval)  # returns a shap.Explanation\n",
        "\n",
        "shap_values.feature_names = feature_names.tolist()\n",
        "\n",
        "# k = 0  # or np.argmax(model.predict(X_eval), axis=1)[i] for per-sample class\n",
        "# shap.plots.beeswarm(shap_values[:, :, k], max_display=5)        # class k\n",
        "\n",
        "# or overall ranking across classes:\n",
        "shap.plots.bar(shap_values.abs.mean(axis=2), max_display=20)     # mean|SHAP| over classes\n",
        "\n",
        "# shap_eavl(X_train, X_test, feature_names)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
